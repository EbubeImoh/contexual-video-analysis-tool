{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading image files using openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying image using opencv, image window won't close until you press 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the image\n",
    "img = cv.imread(cv.samples.findFile(\"lambo.jpg\"))\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if img is None:\n",
    "    sys.exit('Could not read the image.')\n",
    "\n",
    "# Display the image in a window\n",
    "cv.imshow('Display window', img)\n",
    "\n",
    "# Wait for a key press\n",
    "k = cv.waitKey(0)\n",
    "\n",
    "# If 's' is pressed, save the image and close the window\n",
    "if k == ord('s'):\n",
    "    cv.imwrite('lambo.jpg', img)\n",
    "    cv.destroyAllWindows()  # Close the window after saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# def extract_frames(video_path, output_folder):\n",
    "#     video = cv.VideoCapture(video_path)\n",
    "#     count = 0\n",
    "#     while True:\n",
    "#         ret, frame = video.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         cv.imwrite(f\"{output_folder}/frame_{count}.jpg\", frame)\n",
    "#         count += 1\n",
    "#     video.release()\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the video\n",
    "    video = cv.VideoCapture(video_path)\n",
    "\n",
    "    frame_paths = []\n",
    "    frame_index = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break  # Break if no more frames are available\n",
    "\n",
    "        # Define the path to save the frame\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_index}.jpg\")\n",
    "        cv.imwrite(frame_filename, frame)  # Save the frame as a JPEG file\n",
    "        frame_paths.append(frame_filename)  # Store the path of the saved frame\n",
    "        \n",
    "        frame_index += 1\n",
    "\n",
    "    video.release()\n",
    "    \n",
    "    return frame_paths  # Return the list of frame file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frames/frame_0.jpg',\n",
       " 'frames/frame_1.jpg',\n",
       " 'frames/frame_2.jpg',\n",
       " 'frames/frame_3.jpg',\n",
       " 'frames/frame_4.jpg',\n",
       " 'frames/frame_5.jpg',\n",
       " 'frames/frame_6.jpg',\n",
       " 'frames/frame_7.jpg',\n",
       " 'frames/frame_8.jpg',\n",
       " 'frames/frame_9.jpg',\n",
       " 'frames/frame_10.jpg',\n",
       " 'frames/frame_11.jpg',\n",
       " 'frames/frame_12.jpg',\n",
       " 'frames/frame_13.jpg',\n",
       " 'frames/frame_14.jpg',\n",
       " 'frames/frame_15.jpg',\n",
       " 'frames/frame_16.jpg',\n",
       " 'frames/frame_17.jpg',\n",
       " 'frames/frame_18.jpg',\n",
       " 'frames/frame_19.jpg',\n",
       " 'frames/frame_20.jpg',\n",
       " 'frames/frame_21.jpg',\n",
       " 'frames/frame_22.jpg',\n",
       " 'frames/frame_23.jpg',\n",
       " 'frames/frame_24.jpg',\n",
       " 'frames/frame_25.jpg',\n",
       " 'frames/frame_26.jpg',\n",
       " 'frames/frame_27.jpg',\n",
       " 'frames/frame_28.jpg',\n",
       " 'frames/frame_29.jpg',\n",
       " 'frames/frame_30.jpg',\n",
       " 'frames/frame_31.jpg',\n",
       " 'frames/frame_32.jpg',\n",
       " 'frames/frame_33.jpg',\n",
       " 'frames/frame_34.jpg',\n",
       " 'frames/frame_35.jpg',\n",
       " 'frames/frame_36.jpg',\n",
       " 'frames/frame_37.jpg',\n",
       " 'frames/frame_38.jpg',\n",
       " 'frames/frame_39.jpg',\n",
       " 'frames/frame_40.jpg',\n",
       " 'frames/frame_41.jpg',\n",
       " 'frames/frame_42.jpg',\n",
       " 'frames/frame_43.jpg',\n",
       " 'frames/frame_44.jpg',\n",
       " 'frames/frame_45.jpg',\n",
       " 'frames/frame_46.jpg',\n",
       " 'frames/frame_47.jpg',\n",
       " 'frames/frame_48.jpg',\n",
       " 'frames/frame_49.jpg',\n",
       " 'frames/frame_50.jpg',\n",
       " 'frames/frame_51.jpg',\n",
       " 'frames/frame_52.jpg',\n",
       " 'frames/frame_53.jpg',\n",
       " 'frames/frame_54.jpg',\n",
       " 'frames/frame_55.jpg',\n",
       " 'frames/frame_56.jpg',\n",
       " 'frames/frame_57.jpg',\n",
       " 'frames/frame_58.jpg',\n",
       " 'frames/frame_59.jpg',\n",
       " 'frames/frame_60.jpg',\n",
       " 'frames/frame_61.jpg',\n",
       " 'frames/frame_62.jpg',\n",
       " 'frames/frame_63.jpg',\n",
       " 'frames/frame_64.jpg',\n",
       " 'frames/frame_65.jpg',\n",
       " 'frames/frame_66.jpg',\n",
       " 'frames/frame_67.jpg',\n",
       " 'frames/frame_68.jpg',\n",
       " 'frames/frame_69.jpg',\n",
       " 'frames/frame_70.jpg',\n",
       " 'frames/frame_71.jpg',\n",
       " 'frames/frame_72.jpg',\n",
       " 'frames/frame_73.jpg',\n",
       " 'frames/frame_74.jpg',\n",
       " 'frames/frame_75.jpg',\n",
       " 'frames/frame_76.jpg',\n",
       " 'frames/frame_77.jpg',\n",
       " 'frames/frame_78.jpg',\n",
       " 'frames/frame_79.jpg',\n",
       " 'frames/frame_80.jpg',\n",
       " 'frames/frame_81.jpg',\n",
       " 'frames/frame_82.jpg',\n",
       " 'frames/frame_83.jpg',\n",
       " 'frames/frame_84.jpg',\n",
       " 'frames/frame_85.jpg',\n",
       " 'frames/frame_86.jpg',\n",
       " 'frames/frame_87.jpg',\n",
       " 'frames/frame_88.jpg',\n",
       " 'frames/frame_89.jpg',\n",
       " 'frames/frame_90.jpg',\n",
       " 'frames/frame_91.jpg',\n",
       " 'frames/frame_92.jpg',\n",
       " 'frames/frame_93.jpg',\n",
       " 'frames/frame_94.jpg',\n",
       " 'frames/frame_95.jpg',\n",
       " 'frames/frame_96.jpg',\n",
       " 'frames/frame_97.jpg',\n",
       " 'frames/frame_98.jpg',\n",
       " 'frames/frame_99.jpg',\n",
       " 'frames/frame_100.jpg',\n",
       " 'frames/frame_101.jpg',\n",
       " 'frames/frame_102.jpg',\n",
       " 'frames/frame_103.jpg',\n",
       " 'frames/frame_104.jpg',\n",
       " 'frames/frame_105.jpg',\n",
       " 'frames/frame_106.jpg',\n",
       " 'frames/frame_107.jpg',\n",
       " 'frames/frame_108.jpg',\n",
       " 'frames/frame_109.jpg',\n",
       " 'frames/frame_110.jpg',\n",
       " 'frames/frame_111.jpg',\n",
       " 'frames/frame_112.jpg',\n",
       " 'frames/frame_113.jpg',\n",
       " 'frames/frame_114.jpg',\n",
       " 'frames/frame_115.jpg',\n",
       " 'frames/frame_116.jpg',\n",
       " 'frames/frame_117.jpg',\n",
       " 'frames/frame_118.jpg',\n",
       " 'frames/frame_119.jpg',\n",
       " 'frames/frame_120.jpg',\n",
       " 'frames/frame_121.jpg',\n",
       " 'frames/frame_122.jpg',\n",
       " 'frames/frame_123.jpg',\n",
       " 'frames/frame_124.jpg',\n",
       " 'frames/frame_125.jpg',\n",
       " 'frames/frame_126.jpg',\n",
       " 'frames/frame_127.jpg',\n",
       " 'frames/frame_128.jpg',\n",
       " 'frames/frame_129.jpg',\n",
       " 'frames/frame_130.jpg',\n",
       " 'frames/frame_131.jpg',\n",
       " 'frames/frame_132.jpg',\n",
       " 'frames/frame_133.jpg',\n",
       " 'frames/frame_134.jpg',\n",
       " 'frames/frame_135.jpg',\n",
       " 'frames/frame_136.jpg',\n",
       " 'frames/frame_137.jpg',\n",
       " 'frames/frame_138.jpg',\n",
       " 'frames/frame_139.jpg',\n",
       " 'frames/frame_140.jpg',\n",
       " 'frames/frame_141.jpg',\n",
       " 'frames/frame_142.jpg',\n",
       " 'frames/frame_143.jpg',\n",
       " 'frames/frame_144.jpg',\n",
       " 'frames/frame_145.jpg',\n",
       " 'frames/frame_146.jpg',\n",
       " 'frames/frame_147.jpg',\n",
       " 'frames/frame_148.jpg',\n",
       " 'frames/frame_149.jpg',\n",
       " 'frames/frame_150.jpg',\n",
       " 'frames/frame_151.jpg',\n",
       " 'frames/frame_152.jpg',\n",
       " 'frames/frame_153.jpg',\n",
       " 'frames/frame_154.jpg',\n",
       " 'frames/frame_155.jpg',\n",
       " 'frames/frame_156.jpg',\n",
       " 'frames/frame_157.jpg',\n",
       " 'frames/frame_158.jpg',\n",
       " 'frames/frame_159.jpg',\n",
       " 'frames/frame_160.jpg',\n",
       " 'frames/frame_161.jpg',\n",
       " 'frames/frame_162.jpg',\n",
       " 'frames/frame_163.jpg',\n",
       " 'frames/frame_164.jpg',\n",
       " 'frames/frame_165.jpg',\n",
       " 'frames/frame_166.jpg',\n",
       " 'frames/frame_167.jpg',\n",
       " 'frames/frame_168.jpg',\n",
       " 'frames/frame_169.jpg',\n",
       " 'frames/frame_170.jpg',\n",
       " 'frames/frame_171.jpg',\n",
       " 'frames/frame_172.jpg',\n",
       " 'frames/frame_173.jpg',\n",
       " 'frames/frame_174.jpg',\n",
       " 'frames/frame_175.jpg',\n",
       " 'frames/frame_176.jpg',\n",
       " 'frames/frame_177.jpg',\n",
       " 'frames/frame_178.jpg',\n",
       " 'frames/frame_179.jpg',\n",
       " 'frames/frame_180.jpg',\n",
       " 'frames/frame_181.jpg',\n",
       " 'frames/frame_182.jpg',\n",
       " 'frames/frame_183.jpg',\n",
       " 'frames/frame_184.jpg',\n",
       " 'frames/frame_185.jpg',\n",
       " 'frames/frame_186.jpg',\n",
       " 'frames/frame_187.jpg',\n",
       " 'frames/frame_188.jpg',\n",
       " 'frames/frame_189.jpg',\n",
       " 'frames/frame_190.jpg',\n",
       " 'frames/frame_191.jpg',\n",
       " 'frames/frame_192.jpg',\n",
       " 'frames/frame_193.jpg',\n",
       " 'frames/frame_194.jpg',\n",
       " 'frames/frame_195.jpg',\n",
       " 'frames/frame_196.jpg',\n",
       " 'frames/frame_197.jpg',\n",
       " 'frames/frame_198.jpg',\n",
       " 'frames/frame_199.jpg',\n",
       " 'frames/frame_200.jpg',\n",
       " 'frames/frame_201.jpg',\n",
       " 'frames/frame_202.jpg',\n",
       " 'frames/frame_203.jpg',\n",
       " 'frames/frame_204.jpg',\n",
       " 'frames/frame_205.jpg',\n",
       " 'frames/frame_206.jpg',\n",
       " 'frames/frame_207.jpg',\n",
       " 'frames/frame_208.jpg',\n",
       " 'frames/frame_209.jpg',\n",
       " 'frames/frame_210.jpg',\n",
       " 'frames/frame_211.jpg',\n",
       " 'frames/frame_212.jpg',\n",
       " 'frames/frame_213.jpg',\n",
       " 'frames/frame_214.jpg',\n",
       " 'frames/frame_215.jpg',\n",
       " 'frames/frame_216.jpg',\n",
       " 'frames/frame_217.jpg',\n",
       " 'frames/frame_218.jpg',\n",
       " 'frames/frame_219.jpg',\n",
       " 'frames/frame_220.jpg',\n",
       " 'frames/frame_221.jpg',\n",
       " 'frames/frame_222.jpg',\n",
       " 'frames/frame_223.jpg',\n",
       " 'frames/frame_224.jpg',\n",
       " 'frames/frame_225.jpg',\n",
       " 'frames/frame_226.jpg',\n",
       " 'frames/frame_227.jpg',\n",
       " 'frames/frame_228.jpg',\n",
       " 'frames/frame_229.jpg',\n",
       " 'frames/frame_230.jpg',\n",
       " 'frames/frame_231.jpg',\n",
       " 'frames/frame_232.jpg',\n",
       " 'frames/frame_233.jpg',\n",
       " 'frames/frame_234.jpg',\n",
       " 'frames/frame_235.jpg',\n",
       " 'frames/frame_236.jpg',\n",
       " 'frames/frame_237.jpg',\n",
       " 'frames/frame_238.jpg',\n",
       " 'frames/frame_239.jpg',\n",
       " 'frames/frame_240.jpg',\n",
       " 'frames/frame_241.jpg',\n",
       " 'frames/frame_242.jpg',\n",
       " 'frames/frame_243.jpg',\n",
       " 'frames/frame_244.jpg',\n",
       " 'frames/frame_245.jpg',\n",
       " 'frames/frame_246.jpg',\n",
       " 'frames/frame_247.jpg',\n",
       " 'frames/frame_248.jpg',\n",
       " 'frames/frame_249.jpg',\n",
       " 'frames/frame_250.jpg',\n",
       " 'frames/frame_251.jpg',\n",
       " 'frames/frame_252.jpg',\n",
       " 'frames/frame_253.jpg',\n",
       " 'frames/frame_254.jpg',\n",
       " 'frames/frame_255.jpg',\n",
       " 'frames/frame_256.jpg',\n",
       " 'frames/frame_257.jpg',\n",
       " 'frames/frame_258.jpg',\n",
       " 'frames/frame_259.jpg',\n",
       " 'frames/frame_260.jpg',\n",
       " 'frames/frame_261.jpg',\n",
       " 'frames/frame_262.jpg',\n",
       " 'frames/frame_263.jpg',\n",
       " 'frames/frame_264.jpg',\n",
       " 'frames/frame_265.jpg',\n",
       " 'frames/frame_266.jpg',\n",
       " 'frames/frame_267.jpg',\n",
       " 'frames/frame_268.jpg',\n",
       " 'frames/frame_269.jpg',\n",
       " 'frames/frame_270.jpg',\n",
       " 'frames/frame_271.jpg',\n",
       " 'frames/frame_272.jpg',\n",
       " 'frames/frame_273.jpg',\n",
       " 'frames/frame_274.jpg',\n",
       " 'frames/frame_275.jpg',\n",
       " 'frames/frame_276.jpg',\n",
       " 'frames/frame_277.jpg',\n",
       " 'frames/frame_278.jpg',\n",
       " 'frames/frame_279.jpg',\n",
       " 'frames/frame_280.jpg',\n",
       " 'frames/frame_281.jpg',\n",
       " 'frames/frame_282.jpg',\n",
       " 'frames/frame_283.jpg',\n",
       " 'frames/frame_284.jpg',\n",
       " 'frames/frame_285.jpg',\n",
       " 'frames/frame_286.jpg',\n",
       " 'frames/frame_287.jpg',\n",
       " 'frames/frame_288.jpg',\n",
       " 'frames/frame_289.jpg',\n",
       " 'frames/frame_290.jpg',\n",
       " 'frames/frame_291.jpg',\n",
       " 'frames/frame_292.jpg']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_frames('test_video.mp4', 'frames/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifies objects in a scene using RestNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "def classify_frame(frame_path):\n",
    "    img = image.load_img(frame_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    return decode_predictions(preds, top=3)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('n03877845', 'palace', 0.9050565),\n",
       " ('n02817516', 'bearskin', 0.012761978),\n",
       " ('n04005630', 'prison', 0.0058205742)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_frame('frames1/frame_50.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps face to known faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "def recognize_faces(known_faces_encodings, frame_path):\n",
    "    frame = face_recognition.load_image_file(frame_path)\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_faces_encodings, face_encoding)\n",
    "        if True in matches:\n",
    "            match_index = matches.index(True)\n",
    "            # Return matched face details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts audio from video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_audio(video_path, aud_path):\n",
    "    # Load the video\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    # Check if the video has an audio track\n",
    "    if not video.audio:\n",
    "        print(\"No audio track found in the video.\")\n",
    "        return None\n",
    "    \n",
    "    # Save the audio as a WAV file\n",
    "    video.audio.write_audiofile(aud_path, codec='pcm_s16le')\n",
    "    \n",
    "    return aud_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No audio track found in the video.\n"
     ]
    }
   ],
   "source": [
    "extract_audio('test_video2.mp4', 'test_audio2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech recognition algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    try:\n",
    "        return recognizer.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition could not understand the audio\n"
     ]
    }
   ],
   "source": [
    "transcribe_audio('test_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The audio has a lot of noise so this is a cleaning algorithm that transforms the audio to a numpy aray and denoises it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    # Load audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    \n",
    "    # Convert audio to a NumPy array\n",
    "    audio_array = np.array(audio.get_array_of_samples())\n",
    "    \n",
    "    # If stereo, take the first channel (mono)\n",
    "    if audio.channels > 1:\n",
    "        audio_array = audio_array[::audio.channels]\n",
    "    \n",
    "    return audio_array, audio.frame_rate\n",
    "\n",
    "audio_array, sample_rate = load_audio(\"test_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array.dtype.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_noise(audio_array, sample_rate):\n",
    "    # Design a low-pass filter to remove noise\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    low_cutoff = 100.0  # 300 Hz\n",
    "    high_cutoff = 300.0  # 3000 Hz\n",
    "\n",
    "    # Design the filter\n",
    "    b, a = scipy.signal.butter(1, [low_cutoff / nyquist, high_cutoff / nyquist], btype=\"band\")\n",
    "    \n",
    "    # Apply the filter\n",
    "    filtered_audio = scipy.signal.lfilter(b, a, audio_array)\n",
    "    \n",
    "    return filtered_audio\n",
    "\n",
    "cleaned_audio = reduce_noise(audio_array, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(audio_array, sample_rate, output_path):\n",
    "    # Ensure audio_array has a valid dtype for pydub\n",
    "    if audio_array.dtype == np.float32:\n",
    "        audio_array = (audio_array * 32767).astype(np.int16)  # Convert float32 to int16\n",
    "    elif audio_array.dtype == np.float64:\n",
    "        audio_array = (audio_array * 32767).astype(np.int16)  # Convert float64 to int16\n",
    "    elif audio_array.dtype == np.int32:\n",
    "        audio_array = (audio_array // 2).astype(np.int16)  # Convert int32 to int16\n",
    "    elif audio_array.dtype == np.uint8:\n",
    "        audio_array = (audio_array - 128).astype(np.int16)  # Convert uint8 to int16\n",
    "    \n",
    "    sample_width = audio_array.dtype.itemsize  # This should now be valid (1, 2, 3, or 4 bytes)\n",
    "\n",
    "    # Convert NumPy array back to AudioSegment\n",
    "    cleaned_audio_segment = AudioSegment(\n",
    "        audio_array.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=sample_width,\n",
    "        channels=1\n",
    "    )\n",
    "    \n",
    "    # Export the cleaned audio\n",
    "    cleaned_audio_segment.export(output_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_audio = np.random.randn(44100)  # Example audio array\n",
    "save_audio(cleaned_audio, 44100, \"cleaned_test_audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method is very time constraint as i have to tweek the numbers manually until i have a very clear audio...\n",
    "\n",
    "\n",
    "### Trying out the 'noisereduce' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Wave_write.__del__ at 0x1514cd080>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/wave.py\", line 465, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/lib/python3.12/wave.py\", line 583, in close\n",
      "    self._ensure_header_written(0)\n",
      "  File \"/opt/anaconda3/lib/python3.12/wave.py\", line 603, in _ensure_header_written\n",
      "    raise Error('sample width not specified')\n",
      "wave.Error: sample width not specified\n"
     ]
    }
   ],
   "source": [
    "import noisereduce as nr\n",
    "\n",
    "def advanced_noise_reduction(audio_array, sample_rate):\n",
    "    # Apply noise reduction\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio_array, sr=sample_rate)\n",
    "    return reduced_noise_audio\n",
    "\n",
    "cleaned_audio = advanced_noise_reduction(audio_array, sample_rate)\n",
    "save_audio(cleaned_audio, sample_rate, \"cleaned_test_audio_advanced.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting the speech recognition again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition could not understand the audio\n"
     ]
    }
   ],
   "source": [
    "transcribe_audio('cleaned_test_audio_advanced.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>,   â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> â”‚\n",
       "â”‚                                 â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21175</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,200</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m,   â”‚         \u001b[38;5;34m2,624\u001b[0m â”‚\n",
       "â”‚                                 â”‚ \u001b[38;5;34m32\u001b[0m)                    â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling3d_3 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m32\u001b[0m)  â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21175\u001b[0m, \u001b[38;5;34m32\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚        \u001b[38;5;34m53,200\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m101\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,925</span> (218.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,925\u001b[0m (218.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,925</span> (218.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,925\u001b[0m (218.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Conv3D, MaxPooling3D, Reshape, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_action_recognition_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 3D Convolution Layer\n",
    "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(frames, height, width, channels)))\n",
    "    \n",
    "    # 3D MaxPooling Layer\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    \n",
    "    # Reshape Layer to flatten height and width dimensions\n",
    "    model.add(Reshape((-1, 32)))  # Flatten spatial dimensions, keep channels\n",
    "    \n",
    "    # LSTM Layer\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    \n",
    "    # Dense Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "frames = 16  # Number of frames in a sequence\n",
    "height = 112  # Height of each frame\n",
    "width = 112  # Width of each frame\n",
    "channels = 3  # Number of color channels (e.g., RGB)\n",
    "\n",
    "model = build_action_recognition_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/user/Documents/Data Science Projects/Context Understanding from Videos/frames1/frame_100.jpg: 384x640 1 person, 559.8ms\n",
      "Speed: 57.6ms preprocess, 559.8ms inference, 101.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Example: Detect objects in a single frame\n",
    "results = model('frames1/frame_100.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def detect_objects(frames):\n",
    "    # Load YOLOv5 model from Ultralytics\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)\n",
    "    \n",
    "    detected_objects = {}\n",
    "    \n",
    "    for frame_path in frames:\n",
    "        img = Image.open(frame_path)\n",
    "        results = model(img)\n",
    "        detected_objects[frame_path] = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "    \n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "def recognize_faces_in_frames(frames):\n",
    "    recognized_faces = {}\n",
    "    \n",
    "    for frame_path in frames:\n",
    "        image = face_recognition.load_image_file(frame_path)\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "        face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "        recognized_faces[frame_path] = {\n",
    "            'locations': face_locations,\n",
    "            'encodings': face_encodings\n",
    "        }\n",
    "    \n",
    "    return recognized_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer import FER\n",
    "\n",
    "def detect_emotions(frames):\n",
    "    detector = FER()  # Initialize FER detector\n",
    "    detected_emotions = {}\n",
    "    \n",
    "    for frame_path in frames:\n",
    "        # Read the image from the frame path\n",
    "        img = cv.imread(frame_path)\n",
    "        \n",
    "        # Convert image from BGR (OpenCV format) to RGB (FER format)\n",
    "        img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect emotions in the image\n",
    "        result = detector.detect_emotions(img_rgb)\n",
    "        \n",
    "        # Extract the dominant emotion\n",
    "        if result:\n",
    "            emotions = result[0]['emotions']\n",
    "            dominant_emotion = max(emotions, key=emotions.get)\n",
    "        else:\n",
    "            dominant_emotion = \"unknown\"\n",
    "        \n",
    "        detected_emotions[frame_path] = dominant_emotion\n",
    "    \n",
    "    return detected_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_in_frame(known_face_encodings, frame):\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "    matches = face_recognition.compare_faces(known_face_encodings, face_encodings)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.video import r3d_18  # ResNet3D model\n",
    "\n",
    "def recognize_actions(frames):\n",
    "    # Load a pre-trained ResNet3D model for action recognition\n",
    "    model = r3d_18(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define the transformation to apply to each frame\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Prepare a list to store the transformed frames\n",
    "    processed_frames = []\n",
    "    \n",
    "    for frame_path in frames:\n",
    "        img = cv.imread(frame_path)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        transformed_frame = transform(img)\n",
    "        processed_frames.append(transformed_frame)\n",
    "    \n",
    "    # Stack frames along the depth dimension (to match [C, T, H, W])\n",
    "    clip = torch.stack(processed_frames, dim=1).unsqueeze(0)  # Shape: [1, C, T, H, W]\n",
    "    \n",
    "    # Predict actions in the sequence of frames\n",
    "    with torch.no_grad():\n",
    "        outputs = model(clip)\n",
    "    \n",
    "    # Get the predicted action\n",
    "    _, predicted = outputs.max(1)\n",
    "    action_label = predicted.item()  # Convert tensor to integer\n",
    "    \n",
    "    return action_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_context(detected_objects, recognized_faces, detected_emotions, actions, transcribed_text):\n",
    "    # Generate a text-based summary\n",
    "    \n",
    "    # Extract object names from detected_objects\n",
    "    object_names = []\n",
    "    for objs in detected_objects.values():\n",
    "        for obj in objs:\n",
    "            if isinstance(obj, dict):\n",
    "                object_names.append(obj.get('name', 'Unknown Object'))  # Handling if 'name' key is missing\n",
    "            else:\n",
    "                object_names.append(obj)  # In case obj is a string directly\n",
    "\n",
    "    summary = f\"Actions: {actions}\\n\"\n",
    "    summary += f\"Objects Detected: {', '.join(set(object_names))}\\n\"\n",
    "    \n",
    "    # Extract face names from recognized_faces\n",
    "    face_names = []\n",
    "    for faces in recognized_faces.values():\n",
    "        for face in faces:\n",
    "            if isinstance(face, dict):\n",
    "                face_names.append(face.get('name', 'Unknown Face'))  # Handling if 'name' key is missing\n",
    "            else:\n",
    "                face_names.append(face)  # In case face is a string directly\n",
    "    \n",
    "    summary += f\"Faces Recognized: {', '.join(set(face_names))}\\n\"\n",
    "    summary += f\"Emotions Detected: {', '.join(set(detected_emotions.values()))}\\n\"\n",
    "    summary += f\"Transcribed Speech: {transcribed_text}\\n\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio/test_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/user/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ğŸš€ 2024-8-21 Python-3.12.4 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition could not understand the audio\n",
      "Actions: 378\n",
      "Objects Detected: motorcycle, person, dog, car, skateboard, bicycle\n",
      "Faces Recognized: encodings, locations\n",
      "Emotions Detected: happy, fear, unknown, angry, sad\n",
      "Transcribed Speech: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path):\n",
    "    # Step 1: Extract frames and audio\n",
    "    frames = extract_frames(video_path, 'images/frames/')\n",
    "    audio = extract_audio(video_path, 'audio/test_audio.wav')\n",
    "    \n",
    "    # Step 2: Detect objects, faces, emotions in each frame\n",
    "    detected_objects = detect_objects(frames)\n",
    "    recognized_faces = recognize_faces_in_frames(frames)\n",
    "    detected_emotions = detect_emotions(frames)\n",
    "    \n",
    "    # Step 3: Recognize actions in the sequence of frames\n",
    "    actions = recognize_actions(frames)\n",
    "    \n",
    "    # Step 4: Transcribe speech in the audio\n",
    "    transcribed_text = transcribe_audio(audio)\n",
    "    \n",
    "    # Step 5: Contextual analysis (Combine all information)\n",
    "    context = analyze_context(detected_objects, recognized_faces, detected_emotions, actions, transcribed_text)\n",
    "    \n",
    "    # Step 6: Output the context\n",
    "    return context\n",
    "\n",
    "context_summary = analyze_video('video/test_video.mp4')\n",
    "print(context_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
